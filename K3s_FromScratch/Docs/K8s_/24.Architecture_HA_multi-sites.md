ğŸ“˜ Chapitre â€” ğŸ—ï¸ Architecture HA multi-sites Kubernetes (On-Prem, PROD) â€” 10/10 dÃ©taillÃ©

> ğŸ¯ But : concevoir une plateforme tolÃ©rante Ã  la perte dâ€™un site (datacenter A â†” datacenter B), avec un plan de bascule clair, sans â€œmagieâ€ et en restant strictement alignÃ© doc officielle Kubernetes + etcd.

Sources officielles :

HA kubeadm (topologies & exigences)

HA etcd avec kubeadm

Exploitation/upgrade etcd (et plan de sauvegarde)

Kubernetes â€œmultiple zonesâ€ (tolÃ©rance aux pannes de zone / contraintes)

Restauration etcd depuis snapshot (opÃ©rations DR)

---

ğŸ§­ 1) Clarifier le vocabulaire â€œmulti-sitesâ€ (sinon, on se plante)

Zone de dÃ©faillance : groupe de machines pouvant tomber ensemble (rack, salle, DC). Kubernetes sait fonctionner sur plusieurs zones dans un mÃªme cluster si le rÃ©seau le permet.

Site (DC A / DC B) : zone de dÃ©faillance â€œlourdeâ€ (latence + coupure rÃ©seau possible).

HA (High Availability) : continuer Ã  servir mÃªme si un nÅ“ud ou un composant tombe.

DR/PRA (Disaster Recovery / Plan de reprise dâ€™activitÃ©) : reprendre aprÃ¨s perte dâ€™un site entier.

---

# ğŸ§  2) DÃ©cision dâ€™architecture numÃ©ro 1 : 1 cluster Ã©tendu ou 2 clusters ?

Option A â€” Un seul cluster Ã©tendu sur 2 sites

# âœ… Avantage : un seul plan de contrÃ´le, une seule API, une seule â€œvÃ©ritÃ©â€.
âŒ Risque majeur : quorum etcd + latence inter-site + risque de â€œsplit brainâ€ rÃ©seau (partition). etcd est le backing store du cluster et doit Ãªtre opÃ©rÃ© avec un plan de backup sÃ©rieux.

> En pratique, un cluster â€œÃ©tendu multi-sitesâ€ est dÃ©licat. Kubernetes documente bien le multi-zones (dans une mÃªme rÃ©gion/ensemble) mais dÃ¨s que la latence/coupure inter-site devient plausible, il faut Ãªtre extrÃªmement prudent.

Option B â€” Deux clusters (Site A et Site B)

# âœ… Recommandation â€œproduction pragmatiqueâ€ en on-prem multi-sites :

Cluster A = primaire (active)

Cluster B = secondaire (passive) ou active/active selon exigences
# âœ… Avantage : pas de quorum etcd traversant lâ€™inter-site, blast radius rÃ©duit.
# âœ… Bascule = â€œroutage + data replication + GitOpsâ€ (maÃ®trisable).

â¡ï¸ Dans un PRA multi-sites on-prem, lâ€™option B est gÃ©nÃ©ralement la plus robuste.

---

ğŸ—ï¸ 3) Socle HA dans chaque site (non nÃ©gociable)

3.1 Control Plane HA (kubeadm) : 3 nÅ“uds minimum

Kubernetes via kubeadm dÃ©crit 2 topologies HA :

Stacked etcd (etcd co-localisÃ© sur chaque control-plane)

External etcd (etcd sÃ©parÃ©)

ğŸ‘‰ Pour un site â€œprodâ€, tu choisis lâ€™une des deux et tu standardises.

3.2 etcd : la piÃ¨ce la plus critique

etcd stocke tout lâ€™Ã©tat cluster ; il faut un plan de sauvegarde.

Une restauration etcd se fait depuis un snapshot (procÃ©dure DR).

# âœ… Exigence production :

snapshots rÃ©guliers

stockage des snapshots hors site (ou au minimum hors cluster)

test de restauration (sinon Ã§a ne â€œcompteâ€ pas)

---

ğŸŒ 4) EntrÃ©e trafic multi-sites : comment les utilisateurs arrivent Ã  la bonne app ?

Tu as deux modÃ¨les principaux.

ModÃ¨le 1 â€” Active/Passive

Les utilisateurs pointent vers un VIP/DNS qui dirige vers Site A

Si Site A tombe : bascule DNS / GSLB / reverse proxy vers Site B

# âœ… Simple, robuste, trÃ¨s â€œPRAâ€.

ModÃ¨le 2 â€” Active/Active

Les utilisateurs sont rÃ©partis sur A et B (GSLB/DNS geo/latency)

Les deux sites servent en mÃªme temps

Plus complexe : cohÃ©rence donnÃ©es + sessions + dÃ©pendances

# âœ… Plus de disponibilitÃ©/perf, mais plus de complexitÃ©.

---

ğŸ’¾ 5) DonnÃ©es : le vrai cÅ“ur du multi-sites

Kubernetes orchestre les pods, mais la reprise multi-sites dÃ©pend surtout des donnÃ©es.

5.1 Deux familles de donnÃ©es

## 1. Ã‰tat Kubernetes : etcd (manifests, secrets, config, objets)

2) DonnÃ©es applicatives : DB, volumes, fichiers utilisateurs

5.2 StratÃ©gies de persistance multi-sites

DB : rÃ©plication DB (primary/replica) ou cluster DB (selon techno)

Volumes : rÃ©plication storage (SAN/NAS/SDS) ou sauvegarde + restore

Snapshots : indispensables (et testÃ©s)

# ğŸ“Œ Kubernetes insiste sur lâ€™importance dâ€™un plan backup etcd.

---

ğŸ¤– 6) GitOps : la â€œcolonne vertÃ©braleâ€ de la reconstruction

En PRA multi-sites, lâ€™objectif est :

cluster B peut redevenir â€œprodâ€ sans bricolage

tout est reconstruit depuis Git (manifests, Helm values, etc.)

Kubernetes encourage les bonnes pratiques de config versionnÃ©e (Git comme safety net).

---

ğŸ§¯ 7) Runbook de bascule (Active/Passive) â€” modÃ¨le opÃ©rable

DÃ©clencheur

Perte totale Site A (rÃ©seau/Ã©nergie) OU indisponibilitÃ© longue

Ã‰tapes (ordre strict)

## 1. âœ… DÃ©cision : activer PRA (qui, quand, pourquoi)

2) âœ… Bascule trafic : DNS/GSLB/LB vers Site B

## 3. âœ… DonnÃ©es : promotion DB replica â†’ primary (ou restauration)

4) âœ… Kubernetes :

Si Site B a dÃ©jÃ  un cluster prÃªt : appliquer GitOps (sync)

Sinon : reconstruire cluster B (kubeadm HA)

## 5. âœ… Validation : smoke tests (HTTP 200, login, Ã©criture fichier, etc.)

6) âœ… Surveillance : erreurs, saturation, latence

Etcd DR (si reconstruction dâ€™un cluster)

restaurer depuis snapshot etcd si tu veux retrouver lâ€™Ã©tat cluster (sinon GitOps â€œrecrÃ©eâ€)

---

# ğŸ§© 8) Mermaid â€” Architecture HA multi-sites (copiable)

flowchart TB
U[ğŸ‘¤ Utilisateurs] --> DNS[ğŸŒ DNS / GSLB]

DNS -->|Normal| LBA[âš–ï¸ LB / Reverse Proxy - Site A]
DNS -->|Failover| LBB[âš–ï¸ LB / Reverse Proxy - Site B]

- subgraph A["ğŸ¢ Site A (Primary)"]
- direction TB
- subgraph A_CP["â˜¸ï¸ Control Plane HA (A)"]
- A_API[API Endpoint (VIP)]
- A_ETCD[(etcd quorum)]
end
- subgraph A_W["âš™ï¸ Workers (A)"]
- A_ING[ğŸŒ Ingress Controller]
- A_SVC[ğŸ”Œ Services]
- A_PODS[ğŸ“¦ Pods Apps]
end
- A_PODS --> A_PVC[ğŸ“„ PVC]
- A_PVC --> A_CSI[ğŸ”— CSI]
- A_CSI --> A_STOR[ğŸ’¾ Storage (A)]
- A_PODS --> A_DB[(ğŸ—ƒï¸ DB Primary)]
end

- subgraph B["ğŸ¢ Site B (Secondary)"]
- direction TB
- subgraph B_CP["â˜¸ï¸ Control Plane HA (B)"]
- B_API[API Endpoint (VIP)]
- B_ETCD[(etcd quorum)]
end
- subgraph B_W["âš™ï¸ Workers (B)"]
- B_ING[ğŸŒ Ingress Controller]
- B_SVC[ğŸ”Œ Services]
- B_PODS[ğŸ“¦ Pods Apps]
end
- B_PODS --> B_PVC[ğŸ“„ PVC]
- B_PVC --> B_CSI[ğŸ”— CSI]
- B_CSI --> B_STOR[ğŸ’¾ Storage (B)]
- B_PODS --> B_DB[(ğŸ—ƒï¸ DB Replica / DR Target)]
end

- GIT[ğŸ“ Git (SSOT)] --> GITOPS_A[ğŸ¤– GitOps Controller (A)]
- GIT --> GITOPS_B[ğŸ¤– GitOps Controller (B)]
- GITOPS_A --> A_API
- GITOPS_B --> B_API

A_DB -->|Replication| B_DB
A_STOR -->|Replication/Snapshots| B_STOR

---

# âœ… 9) Checklist â€œproduction-readyâ€ multi-sites (trÃ¨s concrÃ¨te)

Dans chaque site

[ ] 3 control-planes (HA kubeadm)

[ ] etcd opÃ©rÃ© + snapshots + tests restore

[ ] Ingress Controller HA

[ ] Stockage via CSI + stratÃ©gie backup

[ ] ObservabilitÃ© + alerting

[ ] GitOps opÃ©rationnel (sync + drift)

Entre sites

[ ] MÃ©canisme de bascule trafic (DNS/GSLB/LB)

[ ] StratÃ©gie â€œdataâ€ (DB + fichiers) documentÃ©e

[ ] Runbook PRA testÃ© (game day)

---

ğŸ”œ Suite (ordre logique que tu as demandÃ©)

## 1. ğŸ§ª Incidents rÃ©els (Pod crash, PVC pending, Ingress KO)

2) ğŸ” Zero Trust (dÃ©jÃ  fait) â†’ on pourra lâ€™appliquer au multi-sites

## 3. ğŸ“¦ Helm + GitOps avancÃ© (dÃ©jÃ  fait) â†’ on lâ€™industrialise (values par site)

4) ğŸ—ï¸ HA multi-sites (fait) â†’ maintenant les scÃ©narios de bascule
