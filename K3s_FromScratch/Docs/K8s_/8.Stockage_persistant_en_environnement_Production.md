ğŸ“˜ Cours Kubernetes (Upstream) â€” Chapitre 8

ğŸ’¾ Stockage persistant en environnement Production On-Premise

---

# ğŸ¯ Objectifs pÃ©dagogiques

Ã€ la fin de ce chapitre, tu comprendras :

Pourquoi le stockage local des nÅ“uds nâ€™est pas suffisant en production

Les architectures de stockage adaptÃ©es Ã  Kubernetes on-prem

Comment Kubernetes consomme un stockage rÃ©seau

Les bonnes pratiques pour bases de donnÃ©es et fichiers applicatifs

---

# ğŸ§  1. Rappel fondamental

Les Pods sont Ã©phÃ©mÃ¨res
Les donnÃ©es applicatives doivent Ãªtre persistantes

En production, la perte dâ€™un nÅ“ud ne doit pas entraÃ®ner la perte des donnÃ©es

ğŸ‘‰ Le stockage doit Ãªtre dÃ©corrÃ©lÃ© des machines Kubernetes

---

ğŸ—ï¸ 2. Architecture de stockage en production on-prem

Pods â†’ PVC â†’ StorageClass â†’ Backend de stockage rÃ©seau â†’ Disques rÃ©els

Le backend de stockage est externe au cluster :

Type Exemple

NAS NFS
- SAN iSCSI / Fibre Channel
- SDS (Software Defined Storage) Ceph, GlusterFS
- Appliance NetApp, Dell EMC, HPE

---

âŒ 3. Pourquoi Ã©viter le stockage local (hostPath)

Risque Explication

- Perte dâ€™un nÅ“ud DonnÃ©es perdues
- Pas de migration Pod liÃ© physiquement au nÅ“ud
- Pas de haute disponibilitÃ© Impossible de rÃ©pliquer facilement

ğŸ‘‰ hostPath = LAB uniquement

---

ğŸŒ 4. Solution standard on-prem : stockage rÃ©seau

Le stockage est fourni par une infrastructure dÃ©diÃ©e, accessible via le rÃ©seau.

Avantages :

Avantage Pourquoi

- IndÃ©pendant des nÅ“uds Les donnÃ©es survivent aux pannes
- Multi-attach possible Plusieurs Pods peuvent lire/Ã©crire
- Sauvegardes centralisÃ©es Snapshot au niveau stockage
- Haute disponibilitÃ© Redondance RAID / cluster stockage

---

# âš™ï¸ 5. IntÃ©gration avec Kubernetes

Kubernetes consomme ce stockage via :

Ã‰lÃ©ment RÃ´le

- CSI Driver Interface entre Kubernetes et le stockage
- StorageClass DÃ©finit le type de stockage
- PVC Demande de volume
- PV Volume crÃ©Ã© cÃ´tÃ© cluster

Documentation officielle :
https://kubernetes.io/docs/concepts/storage/

---

# ğŸ§© 6. Exemple typique : NFS en production

ğŸ—„ï¸ Serveur NFS externe (VM ou baie NAS)

/srv/k8s-data

---

ğŸ“ StorageClass (exemple NFS CSI)

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: nfs-storage
provisioner: nfs.csi.k8s.io
parameters:
- server: 10.10.10.50
- share: /srv/k8s-data
- reclaimPolicy: Retain
- volumeBindingMode: Immediate

---

ğŸ“ PVC

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: app-pvc
spec:
accessModes: - ReadWriteMany
resources:
requests:
storage: 5Gi
storageClassName: nfs-storage

---

ğŸ—ƒï¸ 7. Cas des bases de donnÃ©es

Les bases de donnÃ©es nÃ©cessitent :

Besoin Pourquoi

- Faible latence Performances
- FiabilitÃ© Pas de corruption
- Sauvegardes rÃ©guliÃ¨res Restauration possible
- Snapshots Rollback rapide

ğŸ‘‰ RecommandÃ© : stockage SAN ou SDS (Ceph RBD)

---

# ğŸ§  8. Modes dâ€™accÃ¨s en prod

Mode Usage

- ReadWriteOnce DB (un seul Pod)
- ReadWriteMany CMS, fichiers partagÃ©s
- ReadOnlyMany DonnÃ©es statiques

---

# ğŸ” 9. Bonnes pratiques production

Bonne pratique Pourquoi

- Stockage externe redondÃ© TolÃ©rance aux pannes
- Sauvegardes hors cluster Protection sinistre
- Chiffrement au repos SÃ©curitÃ©
- Monitoring I/O DÃ©tection saturation
- QoS stockage PrioritÃ© aux bases de donnÃ©es

---

ğŸ§ª 10. SchÃ©ma dâ€™architecture type

[Pods]
â†“ PVC
[StorageClass]
â†“ CSI
[Baie de stockage / NAS / Ceph]
â†“ Disques physiques redondÃ©s

---

# ğŸ“Œ RÃ©sumÃ© du chapitre

Ã‰lÃ©ment RÃ´le

- Pods Consomment le stockage
- PVC Demande de volume
- StorageClass Type de stockage
- CSI Driver Interface stockage
- Backend stockage Infrastructure externe

---

ğŸ“ Ã€ retenir

> En production on-premise,
> les donnÃ©es ne doivent jamais dÃ©pendre dâ€™un nÅ“ud Kubernetes.

Le cluster orchestre les applications,
le stockage garantit la durabilitÃ©.

---

ğŸ”œ Chapitre suivant

ğŸ“Š Monitoring & observabilitÃ© (Metrics Server, Prometheus, alerting)
